# this file houses all past and upcoming speakers and the details of their talk and affiliation
#
# TO ADD A SPEAKER: COPY THIS BLOB AND FILL IN THE DETAILS
################################
# - speaker:
#   speakerlink:
#   title:
#   affiliation:
#   abstract:
#   date: yyyy-mm-dd
#   time: 11am
#   location:
#   recording:
################################

# - speaker: Jacob Prince
#   speakerlink: https://jacob-prince.github.io/
#   title: TBD
#   affiliation: Harvard University (PhD Student)
#   abstract: TBD
#   date: 2025-04-28
#   time: 11am
#   location: CIT 477, Lubrano 
#   recording:


# - speaker: Francesca Mignacco
#   speakerlink: https://scholar.google.com/citations?user=maTV19MAAAAJ&hl=en
#   title: TBD
#   affiliation: Princeton/CUNY (Postdoc)
#   abstract: TBD
#   date: 2025-04-14
#   time: 11am
#   location: Carney Innovation Zone 
#   recording:


- speaker: Samuel Lippl
  speakerlink: https://scholar.google.com/citations?user=56QHqZsAAAAJ&hl=en
  title: Emergent mechanisms of compositional generalization
  affiliation: Columbia University Zuckerman Institute (PhD Student)
  abstract: TBD
  date: 2025-03-17
  time: 11am
  location: Metcalf 305 (Dome room) 
  recording:

- speaker: Aditya Yedetore
  speakerlink: https://adityayedetore.github.io/
  title: Classical computation in connectionist models
  affiliation: Boston University Linguistics (PhD student)
  abstract: "The success of Classical (i.e., symbolic) linguistic theories suggests that at least in the linguistic domain, the mind constructs structured symbolic representations and processes them with rules of symbol manipulation. However, modern Connectionist (i.e., neural) models may provide an alternative foundation for linguistic computation: Connectionist models often lack built-in mechanisms for Classical representation and processing, yet they perform impressively on a wide array of linguistic tasks. Connectionist models may not challenge the Classical approach if they implement Classical computers. This is no mere theoretical possibility: when trained on simple symbol manipulation tasks, small Connectionist models develop structured symbolic representations, a key aspect of Classical computation. This raises the possibility that Connectionist models trained on natural data also develop such representations. However, structured representation is not sufficient for Classical computation. The processing of the structured representations by the Connectionist models must involve abstract symbol manipulation of the sort that Classical theories posit. Else, Connectionist models may still challenge the Classical account of human linguistic capacities. We study this question by testing if Connectionist models trained on simple symbol processing tasks develop Classical processing mechanisms. We find evidence suggesting that Connectionist models that succeed on such tasks do implement Classical models. To the extent that such findings generalize to models trained on naturalistic data, such a result would suggest that modern Connectionist models do not challenge Classical theories of human language."
  date: 2025-02-24
  time: 11am
  location: CIT 477, Lubrano
  recording: https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c62c3fa9-f0ee-4704-aa1e-b28e01087705

- speaker: Ekdeep Lubana
  speakerlink: https://ekdeepslubana.github.io/
  title: Dynamics of concept learning and emergent abilities in neural networks
  affiliation: Harvard Kempner Institute (Postdoc)
  abstract: "Neural networks' scaling has been argued to yield sudden learning of capabilities (a.k.a. emergent abilities). In this talk, I will first summarize our recent work on formal models that help explain the mechanisms underlying such sudden learning via data scaling, implicating the compositional nature of a task and formation of structured representations that are shared across several tasks involved in the broader data composition. Then, focusing on in-context learning (ICL)---one such suddenly learned capability---I will demonstrate the precise configurations used for training can lead to learning of fundamentally different algorithms for performing an ICL task. This indicates the phenomenology of ICL established in past work may not be universal. Further, I will discuss how merely scaling the context size can lead to a crossover between different ICL algorithms used by the model. This can be explained via a competition of algorithms lens, which also yields a new theory on the transient nature of ICL."
  date: 2025-02-10
  time: 11am
  location: CIT 477, Lubrano
  recording: https://brown.zoom.us/rec/share/5QpbrWfCiO_D8JA4pb1qT4BEuh6HKRuRuYQiG2YYYWA4IfdjqIQx9-nKx1xgWYhs.dtbpakfqkfV0C_Mo

- speaker: Ciana Deveau
  speakerlink: https://scholar.google.com/citations?user=dhVfNmAAAAAJ&hl=en
  title: Recurrent cortical networks encode natural sensory statistics via sequence ﬁltering
  affiliation: Brown Neuroscience/NIH (PhD student)
  abstract: "Recurrent neural networks can generate dynamics, but in sensory cortex it has been unclear if any dynamic processing is supported by the dense recurrent excitatory-excitatory network. Here we show a new role for recurrent connections in mouse visual cortex: they support powerful dynamical computations, but by filtering sequences of input instead of generating sequences. Using two-photon optogenetics, we measure neural responses to natural images and play them back, finding inputs are amplified when played back during the correct movie dynamic context— when the preceding sequence corresponds to natural vision. This sequence selectivity depends on a network mechanism: earlier input patterns produce responses in other local neurons, which interact with later input patterns. We confirm this mechanism by designing sequences of inputs that are amplified or suppressed by the network. These data suggest recurrent cortical connections perform predictive processing, encoding the statistics of the natural world in input-output transformations."
  date: 2025-01-27
  time: 11am
  location: CIT 477, Lubrano
  recording: 

- speaker: Eghbal Hosseini
  speakerlink: https://eghbalhosseini.github.io/
  title: Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language
  affiliation: MIT Brain+Cognitive Sciences (Postdoc)
  abstract: "Predicting upcoming events is critical to our ability to effectively interact with ourenvironment and conspecifics. In natural language processing, transformer models, which are trained on next-word prediction, appear to construct a general-purposerepresentation of language that can support diverse downstream tasks. However, westill lack an understanding of how a predictive objective shapes such representations. Inspired by recent work in vision neuroscience Hénaff et al.(2019), here we test ahypothesis about predictive representations of autoregressive transformer models. In particular, we test whether the neural trajectory of a sequence of words in asentence becomes progressively more straight as it passes through the layers of thenetwork. The key insight behind this hypothesis is that straighter trajectories shouldfacilitate prediction via linear extrapolation. We quantify straightness using a 1-dimensional curvature metric, and present four findings in support of the trajectorystraightening hypothesis: i) In trained models, the curvature progressively decreasesfrom the first to the middle layers of the network. ii) Models that perform better onthe next-word prediction objective, including larger models and models trained onlarger datasets, exhibit greater decreases in curvature, suggesting that this improvedability to straighten sentence neural trajectories may be the underlying driver ofbetter language modeling performance. iii) Given the same linguistic context, thesequences that are generated by the model have lower curvature than the groundtruth (the actual continuations observed in a language corpus), suggesting thatthe model favors straighter trajectories for making predictions. iv) A consistentrelationship holds between the average curvature and the average surprisal ofsentences in the middle layers of models, such that sentences with straighter neuraltrajectories also have lower surprisal. Importantly, untrained models don’t exhibitthese behaviors. In tandem, these results support the trajectory straighteninghypothesis and provide a possible mechanism for how the geometry of the internalrepresentations of autoregressive models supports next word prediction."
  date: 2024-12-02
  time: 11am
  location: Carney Institute Innovation Zone (4th floor)
  recording: https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=20816c3b-87c8-454c-8207-b23c01120927
