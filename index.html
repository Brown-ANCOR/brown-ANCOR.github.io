<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="ANCOR: AI, Neuro, CogSci Research Talks. Organized at the Department of Computer Science, Carney Center for Computational Brain Science (CCBS), and Department of Cognitive and Psychological Sciences at Brown University.">
  <meta name="keywords" content="ANCOR, AI, Neuroscience, Cognitive Science, research, seminar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ANCOR: AI, Neuro, CogSci Research Talks</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    // function gtag() {
    //   dataLayer.push(arguments);
    // }

    // gtag('js', new Date());

    // gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link
    href="https://fonts.googleapis.com/css2?family=Ubuntu:ital,wght@0,300;0,400;0,500;0,700;1,300;1,400;1,500;1,700&display=swap"
    rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>



</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://brown-ancor.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">⚓ ANCOR: AI<sub>🤖</sub> Neuro<sub>🧠</sub>
              CogSci<sub>⚙</sub> Research talks</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>🤖</sup><a href="https://cs.brown.edu/events/" target="_blank">Department of Computer Science</a>
                &nbsp;&nbsp;
              </span>
              <span class="author-block">
                <sup>🧠</sup><a href="https://ccbs.carney.brown.edu/events" target="_blank">Carney Center for
                  Computational Brain
                  Science (CCBS)
                </a>
                &nbsp;&nbsp;
              </span>
              <span class="author-block">
                <sup>⚙</sup><a href="https://copsy.brown.edu/events" target="_blank">Department of Cognitive and
                  Psychological
                  Sciences
                </a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup><span style="color: brown;">Brown University</span></span>
            </div>
          </div>
        </div>

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Mailing list Link. -->
            <!--
            <span class="link-block">
              <a href="" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-mail"></i>
                </span>
                <span>Sign up for the mailing list! (coming soon)</span>
              </a>
            </span>
            -->
            <!-- Calendar Link. -->
            <span class="link-block">
              <a href="https://calendar.google.com/calendar/embed?src=c_afd71420664ba5d17f5f2918bb58ad7b466a338d17d307cb820c51a10c476ccb%40group.calendar.google.com&ctz=America%2FNew_York"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-google"></i>
                </span>
                <span>google calendar schedule</span>
              </a>
              <a href="https://forms.gle/67GZwjSFQR3YjXs89" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-microphone"></i>
                </span>
                <span>nominate a speaker!</span>
              </a>
            </span>

          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle">
          <b>Fridays at 2pm</b>
          <br>Organized across locations at Brown University
          home to the AI, Neuro, and Cog research communities (check each event for location; click below for map
          links):
          <br>
          
          - <a href="https://maps.app.goo.gl/WPWpT8BrocitU7HbA" target="_blank">Carney Institute Innovation Zone, 164 Angell St (4th ﬂoor)</a>,<br>
          
          - <a href="https://maps.app.goo.gl/f9uriuEiVXWUgWZJ7" target="_blank">CIT 477 (Lubrano), 115 Waterman St</a>,<br>
          
          - <a href="https://maps.app.goo.gl/e8SXR7ZWUXBTAfPT6" target="_blank">Metcalf/CoPsy Department, 190 Thayer St</a>
          
          <br>
          <!-- ANCOR talks are also Carney Center for Computational Brain Science (CCBS) events. -->
        </h2>

      </div>
    </div>
  </section>


  <section class="">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <!-- <h2 class="title is-4">Event schedule</h2> -->
            <div class="columns is-multiline">
              <div class="column is-12">

                <!-- CONTENT EDITED FROM HERE: [a] -->

              </div>
            </div>
  </section>


  <section class="">
    <div class="container is-max-desktop is-max-widescreen">
      <div class="columns is-centered" style="margin: 2%;">

        <!-- Column for future talks -->
        <div class="column is-7">
          <h2 class="title is-4">Upcoming Talks</h2>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <div class="card" style="background-color:lightyellow">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">TBD</p>
                  <p class="is-6">
                    <a href="https://briancheung.github.io/"
                      target="_blank"><b>Brian Cheung</b></a>
                    
                  </p>
                  <p class="is-5">MIT (CBMM Fellow)</p>
                  &nbsp;
                  <p><b>Date:</b> November 14, 2025 (Friday) at 2pm</p>
                  <p><b>Location:</b> Carney Innovation Zone, 164 Angell st, 4th floor</p>
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>TBD</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent.style.display === "none") {
                          abstractContent.style.display = "block";
                          icon.classList.remove('fa-chevron-down');
                          icon.classList.add('fa-chevron-up');
                        } else {
                          abstractContent.style.display = "none";
                          icon.classList.remove('fa-chevron-up');
                          icon.classList.add('fa-chevron-down');
                        }
                      }
                    </script>
                  </div>
                </div>

              </div>
            </div>
          </div>
          &nbsp;
          
          
          
          <div class="card" style="background-color:lightyellow">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">TBD</p>
                  <p class="is-6">
                    <a href="https://eringrant.github.io/"
                      target="_blank"><b>Erin Grant</b></a>
                    
                  </p>
                  <p class="is-5">NYU (Faculty Fellow)</p>
                  &nbsp;
                  <p><b>Date:</b> May 08, 2026 (Friday) at 2pm</p>
                  <p><b>Location:</b> Carney Innovation Zone, 164 Angell st, 4th floor</p>
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>TBD</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent.style.display === "none") {
                          abstractContent.style.display = "block";
                          icon.classList.remove('fa-chevron-down');
                          icon.classList.add('fa-chevron-up');
                        } else {
                          abstractContent.style.display = "none";
                          icon.classList.remove('fa-chevron-up');
                          icon.classList.add('fa-chevron-down');
                        }
                      }
                    </script>
                  </div>
                </div>

              </div>
            </div>
          </div>
          &nbsp;
          
          
        </div>

        <!-- Column for past talks -->
        <div class="column is-7">
          <h2 class="title is-4">Past Talks</h2>
          

          
        

          
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">LLMs represent words, not just tokens</p>
                  <p class="is-6">
                    <a href="https://sfeucht.github.io/"
                      target="_blank"><b>Sheridan Feucht</b></a>
                    
                  </p>
                  <p class="is-5">Northeastern University (PhD Student)</p>
                  &nbsp;
                  <p><b>Date:</b> October 24, 2025 (Friday) at 2pm</p>
                  <p><b>Location:</b> Carney Innovation Zone, 164 Angell st, 4th floor</p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>What is a word to an LLM? Individual tokens are often semantically unrelated to the meanings of the words/concepts they comprise, meaning that LLMs must build up representations of word meaning in-context that are separate from token identities. In this talk, I show how these multi-token words can be used as a wedge to separate conceptual representations from literal token representations. I describe our "dual-route theory of induction", which argues that models copy literal token information in parallel with "fuzzy" word representations, and show that this "fuzzy" concept induction is vital for other tasks, like translation. Finally, I describe our recent work analyzing the geometry of these independent subspaces, which suggests that they are likely useful for more than just copying.
</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Modeling the emergence and function of high-level representations in visual cortex</p>
                  <p class="is-6">
                    <a href="https://jacob-prince.github.io/"
                      target="_blank"><b>Jacob Prince</b></a>
                    
                  </p>
                  <p class="is-5">Harvard University (PhD Student)</p>
                  &nbsp;
                  <p><b>Date:</b> April 28, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> CIT 477, Lubrano</p>
                  
                  <p> <a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=fb03e5eb-4e6a-499e-aaf9-b2d000f6767c" target="_blank" style="color: inherit; text-decoration: underline;">
                      <b>Link to recording</b> </a></p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>How does the visual system develop category-selective regions for faces, bodies, scenes, and words? And how can we tell whether our deep neural network (DNN) models actually capture the feature tuning that arises in these brain areas? In this talk, I will first show that contrastive learning over large-scale natural image sets naturally gives rise to category-selective units for faces, scenes, bodies, and words, even in the absence of any category-specific learning rules or inductive biases. These emergent selective units have dissociable functional roles in object recognition when lesioned, and can predict responses in corresponding selective areas of human ventral visual cortex. These findings support a unifying account of category representation that bridges longstanding debates between modular and distributed theories of high-level vision. Building on this framework, I will then introduce 'parametric neural control' as a novel, more stringent test of DNN-brain alignment. Many DNN encoding models may show near-equal performance in predicting visual responses, while relying on fundamentally different features and computations. We demonstrate this using an interpretability technique called feature accentuation (Hamblin, Fel, et al., 2024), which enables us to synthesize stimulus sets that systematically vary along model-specific encoding axes, and, to then test each model's ability to precisely modulate neural responses in macaque inferotemporal cortex. Strikingly, we find that DNNs with equivalent encoding scores on natural images can show marked differences in their capacity to control neural responses using these targeted image manipulations. This approach therefore provides a new means to arbitrate between models, requiring a stronger commitment to feature tuning properties in local parts of the natural image manifold. Overall, these studies provide an updated deep learning paradigm for understanding the emergence and function of high-level visual representations in greater detail.</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:lightgray">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">(cancelled) Statistical physics of artificial and biological neural networks</p>
                  <p class="is-6">
                    <a href="https://biophysics.princeton.edu/people/francesca-mignacco"
                      target="_blank"><b>Francesca Mignacco</b></a>
                    
                  </p>
                  <p class="is-5">Princeton Center for the Physics of Biological Function (Postdoc)</p>
                  &nbsp;
                  <p><b>Date:</b> April 14, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> Carney Innovation Zone, 164 Angell st, 4th floor</p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>Recent experimental breakthroughs have paved the way for collecting “big” neural datasets through the simultaneous recording of the activity in thousands of neurons. However, our understanding of the fundamental principles governing neural activity at the population level remains sparse and requires the establishment of appropriate theoretical frameworks. In particular, understanding how neural systems process information through high-dimensional representations presents a fundamental open challenge. A parallel issue emerges in the context of artificial neural networks, that operate efficiently via the interaction of billions of artificial neurons. A commonly adopted approach involves the analysis of statistical and geometrical attributes of neural representations as population-level mechanistic descriptors of task implementation. One of these population-geometry metrics is the invariant-object classification capacity. However, this metric has been so far limited to linearly separable settings. In the first part of the talk, I will present a theoretical framework that overcomes this limitation leveraging contextual gating of the input. (Reference: F. Mignacco, C.-N. Chou, and S. Chung. Nonlinear classification of neural manifolds with contextual information. Physical Review E 111.3 (2025): 035302).  Training machine learning models relies on various optimization strategies to enhance performance. These include optimization algorithms with adaptive hyper-parameters, time-dependent selection of training examples, and model refinement through dynamic architectures. While these strategies aim to accelerate training and steer models toward solutions with good generalization properties, they often rely on trial-and-error heuristics and lack a solid theoretical foundation. Furthermore, machine learning problems are inherently high-dimensional—in terms of dataset size, input dimensions, and model parameters—challenging meta-optimization techniques that can suffer from the curse of dimensionality. Recent advances in the statistical physics of neural networks have provided powerful tools to capture high-dimensional training dynamics through low-dimensional effective equations that track the evolution of key order parameters. In the second part of the talk, I will present how to integrate dimensionality-reduction techniques from statistical physics with control-theoretic methods to identify optimal training strategies, focusing on continual learning.  (Reference: F. Mori, S. Sarao Mannelli, F. Mignacco. Optimal protocols for continual learning via statistical physics and control theory. arXiv preprint arXiv:2409.18061 (2024). Accepted at ICLR 2025).</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Emergent mechanisms of compositional generalization</p>
                  <p class="is-6">
                    <a href="https://scholar.google.com/citations?user=56QHqZsAAAAJ&hl=en"
                      target="_blank"><b>Samuel Lippl</b></a>
                    
                  </p>
                  <p class="is-5">Columbia University Zuckerman Institute (PhD Student)</p>
                  &nbsp;
                  <p><b>Date:</b> March 17, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> Metcalf 107</p>
                  
                  <p> <a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6e330e59-b3b7-4ce6-99c2-b2d000f6746d" target="_blank" style="color: inherit; text-decoration: underline;">
                      <b>Link to recording</b> </a></p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>TBD</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Classical computation in connectionist models</p>
                  <p class="is-6">
                    <a href="https://adityayedetore.github.io/"
                      target="_blank"><b>Aditya Yedetore</b></a>
                    
                  </p>
                  <p class="is-5">Boston University Linguistics (PhD student)</p>
                  &nbsp;
                  <p><b>Date:</b> February 24, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> CIT 477, Lubrano</p>
                  
                  <p> <a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c62c3fa9-f0ee-4704-aa1e-b28e01087705" target="_blank" style="color: inherit; text-decoration: underline;">
                      <b>Link to recording</b> </a></p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>The success of Classical (i.e., symbolic) linguistic theories suggests that at least in the linguistic domain, the mind constructs structured symbolic representations and processes them with rules of symbol manipulation. However, modern Connectionist (i.e., neural) models may provide an alternative foundation for linguistic computation: Connectionist models often lack built-in mechanisms for Classical representation and processing, yet they perform impressively on a wide array of linguistic tasks. Connectionist models may not challenge the Classical approach if they implement Classical computers. This is no mere theoretical possibility: when trained on simple symbol manipulation tasks, small Connectionist models develop structured symbolic representations, a key aspect of Classical computation. This raises the possibility that Connectionist models trained on natural data also develop such representations. However, structured representation is not sufficient for Classical computation. The processing of the structured representations by the Connectionist models must involve abstract symbol manipulation of the sort that Classical theories posit. Else, Connectionist models may still challenge the Classical account of human linguistic capacities. We study this question by testing if Connectionist models trained on simple symbol processing tasks develop Classical processing mechanisms. We find evidence suggesting that Connectionist models that succeed on such tasks do implement Classical models. To the extent that such findings generalize to models trained on naturalistic data, such a result would suggest that modern Connectionist models do not challenge Classical theories of human language.</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Dynamics of concept learning and emergent abilities in neural networks</p>
                  <p class="is-6">
                    <a href="https://ekdeepslubana.github.io/"
                      target="_blank"><b>Ekdeep Lubana</b></a>
                    
                  </p>
                  <p class="is-5">Harvard Kempner Institute (Postdoc)</p>
                  &nbsp;
                  <p><b>Date:</b> February 10, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> CIT 477, Lubrano</p>
                  
                  <p> <a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e75c1379-47ad-44d1-913b-b2d000f74df0" target="_blank" style="color: inherit; text-decoration: underline;">
                      <b>Link to recording</b> </a></p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>Neural networks' scaling has been argued to yield sudden learning of capabilities (a.k.a. emergent abilities). In this talk, I will first summarize our recent work on formal models that help explain the mechanisms underlying such sudden learning via data scaling, implicating the compositional nature of a task and formation of structured representations that are shared across several tasks involved in the broader data composition. Then, focusing on in-context learning (ICL)---one such suddenly learned capability---I will demonstrate the precise configurations used for training can lead to learning of fundamentally different algorithms for performing an ICL task. This indicates the phenomenology of ICL established in past work may not be universal. Further, I will discuss how merely scaling the context size can lead to a crossover between different ICL algorithms used by the model. This can be explained via a competition of algorithms lens, which also yields a new theory on the transient nature of ICL.</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Recurrent cortical networks encode natural sensory statistics via sequence ﬁltering</p>
                  <p class="is-6">
                    <a href="https://scholar.google.com/citations?user=dhVfNmAAAAAJ&hl=en"
                      target="_blank"><b>Ciana Deveau</b></a>
                    
                  </p>
                  <p class="is-5">Brown Neuroscience/NIH (PhD student)</p>
                  &nbsp;
                  <p><b>Date:</b> January 27, 2025 (Monday) at 11am</p>
                  <p><b>Location:</b> CIT 477, Lubrano</p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>Recurrent neural networks can generate dynamics, but in sensory cortex it has been unclear if any dynamic processing is supported by the dense recurrent excitatory-excitatory network. Here we show a new role for recurrent connections in mouse visual cortex: they support powerful dynamical computations, but by filtering sequences of input instead of generating sequences. Using two-photon optogenetics, we measure neural responses to natural images and play them back, finding inputs are amplified when played back during the correct movie dynamic context— when the preceding sequence corresponds to natural vision. This sequence selectivity depends on a network mechanism: earlier input patterns produce responses in other local neurons, which interact with later input patterns. We confirm this mechanism by designing sequences of inputs that are amplified or suppressed by the network. These data suggest recurrent cortical connections perform predictive processing, encoding the statistics of the natural world in input-output transformations.</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        

           <div class="card"
            style="background-color:blanchedalmond">
            <div class="card-content">
              <div class="media">
                <div class="media-content">
                  <p class="title is-4">Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language</p>
                  <p class="is-6">
                    <a href="https://eghbalhosseini.github.io/"
                      target="_blank"><b>Eghbal Hosseini</b></a>
                    
                  </p>
                  <p class="is-5">MIT Brain+Cognitive Sciences (Postdoc)</p>
                  &nbsp;
                  <p><b>Date:</b> December 02, 2024 (Monday) at 11am</p>
                  <p><b>Location:</b> Carney Institute Innovation Zone (4th floor)</p>
                  
                  <p> <a href="https://brown.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=20816c3b-87c8-454c-8207-b23c01120927" target="_blank" style="color: inherit; text-decoration: underline;">
                      <b>Link to recording</b> </a></p>
                  
                  <div class="content">
                    <p><b>Abstract:</b>
                      <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                        <span class="icon">
                          <i class="fas fa-chevron-down"></i>
                        </span>
                      </a>
                    </p>
                    <div class="abstract-content" style="display: none;">
                      <p>Predicting upcoming events is critical to our ability to effectively interact with ourenvironment and conspecifics. In natural language processing, transformer models, which are trained on next-word prediction, appear to construct a general-purposerepresentation of language that can support diverse downstream tasks. However, westill lack an understanding of how a predictive objective shapes such representations. Inspired by recent work in vision neuroscience Hénaff et al.(2019), here we test ahypothesis about predictive representations of autoregressive transformer models. In particular, we test whether the neural trajectory of a sequence of words in asentence becomes progressively more straight as it passes through the layers of thenetwork. The key insight behind this hypothesis is that straighter trajectories shouldfacilitate prediction via linear extrapolation. We quantify straightness using a 1-dimensional curvature metric, and present four findings in support of the trajectorystraightening hypothesis: i) In trained models, the curvature progressively decreasesfrom the first to the middle layers of the network. ii) Models that perform better onthe next-word prediction objective, including larger models and models trained onlarger datasets, exhibit greater decreases in curvature, suggesting that this improvedability to straighten sentence neural trajectories may be the underlying driver ofbetter language modeling performance. iii) Given the same linguistic context, thesequences that are generated by the model have lower curvature than the groundtruth (the actual continuations observed in a language corpus), suggesting thatthe model favors straighter trajectories for making predictions. iv) A consistentrelationship holds between the average curvature and the average surprisal ofsentences in the middle layers of models, such that sentences with straighter neuraltrajectories also have lower surprisal. Importantly, untrained models don’t exhibitthese behaviors. In tandem, these results support the trajectory straighteninghypothesis and provide a possible mechanism for how the geometry of the internalrepresentations of autoregressive models supports next word prediction.</p>
                    </div>

                    <script>
                      function toggleAbstract(link) {
                        const abstractContent = link.parentElement?.nextElementSibling;
                        const icon = link.querySelector('.fa-chevron-down, .fa-chevron-up');
                        if (abstractContent && icon) {
                          if (abstractContent.style.display === "none") {
                            abstractContent.style.display = "block";
                            icon.classList.remove('fa-chevron-down');
                            icon.classList.add('fa-chevron-up');
                          } else {
                            abstractContent.style.display = "none";
                            icon.classList.remove('fa-chevron-up');
                            icon.classList.add('fa-chevron-down');
                          }
                        }
                      }
                    </script>
                  </div>
                </div>
              </div>
            </div>
        </div>
        &nbsp;
        
        
      </div>
    </div>
  </section>




  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>ANCOR is organized by
              
              <b>Aalok Sathe</b> (aalok [at] brown [dot] edu)  &nbsp;&&nbsp; 
              
              <b>Mikey Lepori</b> (michael_lepori [at] brown [dot] edu) 
              
              <br>
              Want to help organize ANCOR? Please reach out!
            </p>
            <p class="is-size-7">
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
              This means you are free to borrow the <a href="https://github.com/brown-ancor/brown-ancor.github.io/"
                target="_blank">source
                code</a> of this website, we just ask that you link back to this page in the footer. (Originally adapted
              from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>).
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>