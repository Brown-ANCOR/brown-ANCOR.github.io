<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="ANCOR: AI, Neuro, CogSci Research Talks. Organized at the Department of Computer Science, Carney Center for Computational Brain Science (CCBS), and Department of Cognitive and Psychological Sciences at Brown University.">
  <meta name="keywords" content="ANCOR, AI, Neuroscience, Cognitive Science, research, seminar">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ANCOR: AI, Neuro, CogSci Research Talks</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>



</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://brown-ancor.github.io">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <!-- <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div> -->
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">⚓ ANCOR: AI<sub>🤖</sub> Neuro<sub>🧠</sub>
              CogSci<sub>⚙</sub> Research talks</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>🤖</sup><a href="https://cs.brown.edu/events/" target="_blank">Department of Computer Science</a>
                &nbsp;&nbsp;
              </span>
              <span class="author-block">
                <sup>🧠</sup><a href="https://ccbs.carney.brown.edu/events" target="_blank">Carney Center for
                  Computational Brain
                  Science (CCBS)
                </a>
                &nbsp;&nbsp;
              </span>
              <span class="author-block">
                <sup>⚙</sup><a href="https://copsy.brown.edu/events" target="_blank">Department of Cognitive and
                  Psychological
                  Sciences
                </a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup></sup><span style="color: brown;">Brown University</span></span>
            </div>
          </div>
        </div>

        <div class="column has-text-centered">
          <div class="publication-links">
            <!-- Mailing list Link. -->
            <span class="link-block">
              <a href="" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-mail"></i>
                </span>
                <span>Sign up for the mailing list! (coming soon)</span>
              </a>
            </span>
            <!-- Calendar Link. -->
            <span class="link-block">
              <a href="" class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-google"></i>
                </span>
                <span>Google calendar (coming soon)</span>
              </a>
            </span>

          </div>
        </div>
      </div>
    </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle">
          <b>Mondays at 11am</b>
          <br>Organized across locations at Brown University
          home to the AI, Neuro, and Cog research communities (check each event for location; click below for map
          links):
          <br>
          
          - <a href="https://maps.app.goo.gl/f9uriuEiVXWUgWZJ7" target="_blank">CIT 477 (Lubrano), 115 Waterman st</a>,<br>
          
          - <a href="https://maps.app.goo.gl/WPWpT8BrocitU7HbA" target="_blank">Carney Institute Innovation Zone, 164 Angell st. 4th ﬂoor</a>,<br>
          
          - <a href="https://maps.app.goo.gl/e8SXR7ZWUXBTAfPT6" target="_blank">Metcalf Research Building (room tbd)</a>
          
          <br>
          <!-- ANCOR talks are also Carney Center for Computational Brain Science (CCBS) events. -->
        </h2>
        <h3>Organized by <br>
          
          <b>Mikey Lepori</b> (michael_lepori [at] brown [dot] edu)  &nbsp;&&nbsp; 
          
          <b>Aalok Sathe</b> (aalok [at] brown [dot] edu) 
          
        </h3>
      </div>
    </div>
  </section>


  <section class="">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column is-10">
          <div class="content">
            <h2 class="title is-4">Event schedule</h2>
            <div class="columns is-multiline">
              <div class="column is-12">

                


                <div class="card" style="background-color:blanchedalmond">
                  <div class="card-content">
                    <div class="media">
                      <div class="media-content">
                        <p class="title is-4">Semantic Bootstrapping of Hierarchical Generalization in Neural Networks</p>
                        <!-- </div>
                      <div>&nbsp;</div>
                      <div> -->
                        <p class=" is-5"><b>Speaker:</b>
                          <a href="https://adityayedetore.github.io/"
                            target="_blank"><b>Aditya Yedetore,</b></a>
                          
                          Boston University (PhD student)
                        </p>
                        <!-- <p class="subtitle is-6">TBA</p> -->
                      </div>
                    </div>
                    <div class="content">
                      <p><b>Date:</b> February 24, 2025 (Monday) at 11am</p>
                      <p><b>Location:</b> CIT 477, Lubrano</p>
                      
                      <div class="content">
                        <div class="abstract-content" style="display: none;">
                          <p>Neural networks without hierarchical biases often struggle to learn linguistic rules that come naturally to humans. However, neural networks are trained primarily on form alone, while children acquiring language additionally receive data about meaning. Would neural networks generalize more like humans when trained on both form and meaning? We investigate this by examining if Transformers—neural networks without a hierarchical bias—better achieve hierarchical generalization when trained on both form and meaning compared to when trained on form alone. Our results show that Transformers trained on form and meaning do favor the hierarchical generalization more than those trained on form alone, suggesting that statistical learners without hierarchical biases can leverage semantic training signals to bootstrap hierarchical syntactic generalization.</p>
                        </div>
                        <p><b>Abstract:</b>
                          <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                            <span class="icon">
                              <i class="fas fa-chevron-down"></i>
                            </span>
                          </a>
                        </p>
                        <div class="abstract-content" style="display: none;">
                          <p>Neural networks without hierarchical biases often struggle to learn linguistic rules that come naturally to humans. However, neural networks are trained primarily on form alone, while children acquiring language additionally receive data about meaning. Would neural networks generalize more like humans when trained on both form and meaning? We investigate this by examining if Transformers—neural networks without a hierarchical bias—better achieve hierarchical generalization when trained on both form and meaning compared to when trained on form alone. Our results show that Transformers trained on form and meaning do favor the hierarchical generalization more than those trained on form alone, suggesting that statistical learners without hierarchical biases can leverage semantic training signals to bootstrap hierarchical syntactic generalization.</p>
                        </div>

                        <script>
                          function toggleAbstract(link) {
                            const abstractContent = link.parentElement.nextElementSibling;
                            const icon = link.querySelector('.icon i');
                            if (abstractContent.style.display === "none") {
                              abstractContent.style.display = "block";
                              icon.classList.remove('fa-chevron-down');
                              icon.classList.add('fa-chevron-up');
                            } else {
                              abstractContent.style.display = "none";
                              icon.classList.remove('fa-chevron-up');
                              icon.classList.add('fa-chevron-down');
                            }
                          }
                        </script>
                      </div>
                    </div>
                  </div>
                </div>
                <!-- Finish event card -->
                <div style="margin-bottom: 20px;"></div>
                


                <div class="card" style="background-color:blanchedalmond">
                  <div class="card-content">
                    <div class="media">
                      <div class="media-content">
                        <p class="title is-4">Dynamics of Concept Learning and Emergent Abilities in Neural Networks</p>
                        <!-- </div>
                      <div>&nbsp;</div>
                      <div> -->
                        <p class=" is-5"><b>Speaker:</b>
                          <a href="https://ekdeepslubana.github.io/"
                            target="_blank"><b>Ekdeep Lubana,</b></a>
                          
                          Harvard Kempner Institute (Postdoc)
                        </p>
                        <!-- <p class="subtitle is-6">TBA</p> -->
                      </div>
                    </div>
                    <div class="content">
                      <p><b>Date:</b> February 10, 2025 (Monday) at 11am</p>
                      <p><b>Location:</b> CIT 477, Lubrano</p>
                      
                      <div class="content">
                        <div class="abstract-content" style="display: none;">
                          <p>None</p>
                        </div>
                        <p><b>Abstract:</b>
                          <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                            <span class="icon">
                              <i class="fas fa-chevron-down"></i>
                            </span>
                          </a>
                        </p>
                        <div class="abstract-content" style="display: none;">
                          <p>None</p>
                        </div>

                        <script>
                          function toggleAbstract(link) {
                            const abstractContent = link.parentElement.nextElementSibling;
                            const icon = link.querySelector('.icon i');
                            if (abstractContent.style.display === "none") {
                              abstractContent.style.display = "block";
                              icon.classList.remove('fa-chevron-down');
                              icon.classList.add('fa-chevron-up');
                            } else {
                              abstractContent.style.display = "none";
                              icon.classList.remove('fa-chevron-up');
                              icon.classList.add('fa-chevron-down');
                            }
                          }
                        </script>
                      </div>
                    </div>
                  </div>
                </div>
                <!-- Finish event card -->
                <div style="margin-bottom: 20px;"></div>
                


                <div class="card" style="background-color:blanchedalmond">
                  <div class="card-content">
                    <div class="media">
                      <div class="media-content">
                        <p class="title is-4">Recurrent cortical networks encode natural sensory statistics via sequence ﬁltering</p>
                        <!-- </div>
                      <div>&nbsp;</div>
                      <div> -->
                        <p class=" is-5"><b>Speaker:</b>
                          <a href="https://scholar.google.com/citations?user=dhVfNmAAAAAJ&hl=en"
                            target="_blank"><b>Ciana Deveau,</b></a>
                          
                          Brown Neuroscience/NIH (PhD student)
                        </p>
                        <!-- <p class="subtitle is-6">TBA</p> -->
                      </div>
                    </div>
                    <div class="content">
                      <p><b>Date:</b> January 27, 2025 (Monday) at 11am</p>
                      <p><b>Location:</b> CIT 477, Lubrano</p>
                      
                      <div class="content">
                        <div class="abstract-content" style="display: none;">
                          <p>Recurrent neural networks can generate dynamics, but in sensory cortex it has been unclear if any dynamic processing is supported by the dense recurrent excitatory-excitatory network. Here we show a new role for recurrent connections in mouse visual cortex: they support powerful dynamical computations, but by filtering sequences of input instead of generating sequences. Using two-photon optogenetics, we measure neural responses to natural images and play them back, finding inputs are amplified when played back during the correct movie dynamic context— when the preceding sequence corresponds to natural vision. This sequence selectivity depends on a network mechanism: earlier input patterns produce responses in other local neurons, which interact with later input patterns. We confirm this mechanism by designing sequences of inputs that are amplified or suppressed by the network. These data suggest recurrent cortical connections perform predictive processing, encoding the statistics of the natural world in input-output transformations.</p>
                        </div>
                        <p><b>Abstract:</b>
                          <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                            <span class="icon">
                              <i class="fas fa-chevron-down"></i>
                            </span>
                          </a>
                        </p>
                        <div class="abstract-content" style="display: none;">
                          <p>Recurrent neural networks can generate dynamics, but in sensory cortex it has been unclear if any dynamic processing is supported by the dense recurrent excitatory-excitatory network. Here we show a new role for recurrent connections in mouse visual cortex: they support powerful dynamical computations, but by filtering sequences of input instead of generating sequences. Using two-photon optogenetics, we measure neural responses to natural images and play them back, finding inputs are amplified when played back during the correct movie dynamic context— when the preceding sequence corresponds to natural vision. This sequence selectivity depends on a network mechanism: earlier input patterns produce responses in other local neurons, which interact with later input patterns. We confirm this mechanism by designing sequences of inputs that are amplified or suppressed by the network. These data suggest recurrent cortical connections perform predictive processing, encoding the statistics of the natural world in input-output transformations.</p>
                        </div>

                        <script>
                          function toggleAbstract(link) {
                            const abstractContent = link.parentElement.nextElementSibling;
                            const icon = link.querySelector('.icon i');
                            if (abstractContent.style.display === "none") {
                              abstractContent.style.display = "block";
                              icon.classList.remove('fa-chevron-down');
                              icon.classList.add('fa-chevron-up');
                            } else {
                              abstractContent.style.display = "none";
                              icon.classList.remove('fa-chevron-up');
                              icon.classList.add('fa-chevron-down');
                            }
                          }
                        </script>
                      </div>
                    </div>
                  </div>
                </div>
                <!-- Finish event card -->
                <div style="margin-bottom: 20px;"></div>
                


                <div class="card" style="background-color:blanchedalmond">
                  <div class="card-content">
                    <div class="media">
                      <div class="media-content">
                        <p class="title is-4">Large language models implicitly learn to straighten neural sentence trajectories to construct a predictive representation of natural language</p>
                        <!-- </div>
                      <div>&nbsp;</div>
                      <div> -->
                        <p class=" is-5"><b>Speaker:</b>
                          <a href="https://eghbalhosseini.github.io/"
                            target="_blank"><b>Eghbal Hosseini,</b></a>
                          
                          MIT Brain+Cognitive Sciences (Postdoc)
                        </p>
                        <!-- <p class="subtitle is-6">TBA</p> -->
                      </div>
                    </div>
                    <div class="content">
                      <p><b>Date:</b> December 02, 2024 (Monday) at 11am</p>
                      <p><b>Location:</b> Carney Institute Innovation Zone (4th floor)</p>
                      
                      <div class="content">
                        <div class="abstract-content" style="display: none;">
                          <p>Predicting upcoming events is critical to our ability to effectively interact with ourenvironment and conspecifics. In natural language processing, transformer models, which are trained on next-word prediction, appear to construct a general-purposerepresentation of language that can support diverse downstream tasks. However, westill lack an understanding of how a predictive objective shapes such representations. Inspired by recent work in vision neuroscience Hénaff et al.(2019), here we test ahypothesis about predictive representations of autoregressive transformer models. In particular, we test whether the neural trajectory of a sequence of words in asentence becomes progressively more straight as it passes through the layers of thenetwork. The key insight behind this hypothesis is that straighter trajectories shouldfacilitate prediction via linear extrapolation. We quantify straightness using a 1-dimensional curvature metric, and present four findings in support of the trajectorystraightening hypothesis: i) In trained models, the curvature progressively decreasesfrom the first to the middle layers of the network. ii) Models that perform better onthe next-word prediction objective, including larger models and models trained onlarger datasets, exhibit greater decreases in curvature, suggesting that this improvedability to straighten sentence neural trajectories may be the underlying driver ofbetter language modeling performance. iii) Given the same linguistic context, thesequences that are generated by the model have lower curvature than the groundtruth (the actual continuations observed in a language corpus), suggesting thatthe model favors straighter trajectories for making predictions. iv) A consistentrelationship holds between the average curvature and the average surprisal ofsentences in the middle layers of models, such that sentences with straighter neuraltrajectories also have lower surprisal. Importantly, untrained models don’t exhibitthese behaviors. In tandem, these results support the trajectory straighteninghypothesis and provide a possible mechanism for how the geometry of the internalrepresentations of autoregressive models supports next word prediction.</p>
                        </div>
                        <p><b>Abstract:</b>
                          <a href="javascript:void(0);" onclick="toggleAbstract(this)">
                            <span class="icon">
                              <i class="fas fa-chevron-down"></i>
                            </span>
                          </a>
                        </p>
                        <div class="abstract-content" style="display: none;">
                          <p>Predicting upcoming events is critical to our ability to effectively interact with ourenvironment and conspecifics. In natural language processing, transformer models, which are trained on next-word prediction, appear to construct a general-purposerepresentation of language that can support diverse downstream tasks. However, westill lack an understanding of how a predictive objective shapes such representations. Inspired by recent work in vision neuroscience Hénaff et al.(2019), here we test ahypothesis about predictive representations of autoregressive transformer models. In particular, we test whether the neural trajectory of a sequence of words in asentence becomes progressively more straight as it passes through the layers of thenetwork. The key insight behind this hypothesis is that straighter trajectories shouldfacilitate prediction via linear extrapolation. We quantify straightness using a 1-dimensional curvature metric, and present four findings in support of the trajectorystraightening hypothesis: i) In trained models, the curvature progressively decreasesfrom the first to the middle layers of the network. ii) Models that perform better onthe next-word prediction objective, including larger models and models trained onlarger datasets, exhibit greater decreases in curvature, suggesting that this improvedability to straighten sentence neural trajectories may be the underlying driver ofbetter language modeling performance. iii) Given the same linguistic context, thesequences that are generated by the model have lower curvature than the groundtruth (the actual continuations observed in a language corpus), suggesting thatthe model favors straighter trajectories for making predictions. iv) A consistentrelationship holds between the average curvature and the average surprisal ofsentences in the middle layers of models, such that sentences with straighter neuraltrajectories also have lower surprisal. Importantly, untrained models don’t exhibitthese behaviors. In tandem, these results support the trajectory straighteninghypothesis and provide a possible mechanism for how the geometry of the internalrepresentations of autoregressive models supports next word prediction.</p>
                        </div>

                        <script>
                          function toggleAbstract(link) {
                            const abstractContent = link.parentElement.nextElementSibling;
                            const icon = link.querySelector('.icon i');
                            if (abstractContent.style.display === "none") {
                              abstractContent.style.display = "block";
                              icon.classList.remove('fa-chevron-down');
                              icon.classList.add('fa-chevron-up');
                            } else {
                              abstractContent.style.display = "none";
                              icon.classList.remove('fa-chevron-up');
                              icon.classList.add('fa-chevron-down');
                            }
                          }
                        </script>
                      </div>
                    </div>
                  </div>
                </div>
                <!-- Finish event card -->
                <div style="margin-bottom: 20px;"></div>
                

              </div>
            </div>
  </section>


  <footer class="footer">
    <div class="container">
      <!-- <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div> -->
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>

              This means you are free to borrow the <a href="https://github.com/brown-ancor/brown-ancor.github.io/"
                target="_blank">source
                code</a> of this website, we just ask that you link back to this page in the footer. (Originally adapted
              from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>).
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>